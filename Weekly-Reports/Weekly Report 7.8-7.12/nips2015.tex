\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{algorithm}  
\usepackage{algorithmic}
\usepackage[colorlinks,linkcolor=blue]{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{amssymb}
\usepackage{amsmath}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Weekly Report(July 12th, 2019)
}


\author{
David S.~Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Jianghao Lin \\
Shanghai Jiao Tong University \\
\texttt{chiangel.ljh@gmail.com} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
I read the paper \emph{Generative Adversarial Networks} written by Ian Goodfellow in 2014 and figure out the basic idea and mathematical derivations of GAN.
\end{abstract}

\section{Basic idea of GAN}

Generative adversarial nets are based on a game theoretic scenario where the generator network directly produces images $x$ from a random noise $z$ and another discriminator network tries to distinguish the generated image from the real ones.

Ideally, the distribution of the generated data will be the same as the real data image and the accuracy of the discriminator network will be $0.5$ because it can not tell an image is actually fake or not.

\section{Mathematical derivations}

\subsection{How to measure the divergence ?}

\subsubsection{KL divergence}

KL divergence is also called relative entropy. 

For discrete probability distributions $P$ and $Q$ defined on the same probability space, the KL divergence between $P$ and $Q$ is defined to be:

\begin{equation}
    KL(P||Q) = \sum_{x}P(x)log(\frac{P(x)}{Q(x)})
\end{equation}

For continuous random variables, it is defined to be:

\begin{equation}
    KL(P||Q) = \int_{-\infty}^{\infty}p(x)log(\frac{p(x)}{q(x)})dx
\end{equation}

We can have the inequation because the log function is a convex function: 

\begin{equation}
    \begin{split}
        KL(P||Q) & = \int_{-\infty}^{\infty}p(x)log(\frac{p(x)}{q(x)})dx \\
                 & = -E(log\frac{P(x)}{Q(x)}) \\
                 & \geq -log(E(\frac{P(x)}{Q(x)})) \\
                 & = -log(P(x) \times \frac{P(x)}{Q(x)}) = 0
    \end{split}
\end{equation}

Therefore, KL divergence is in range of $[0, 1]$. KL divergence is equal to $0$ if and only if two distribution is exactly the same. And the smaller the KL divergence is, the more similar two distributions are.

\subsubsection{JS divergence}

It is obvious that KL divergence is asymmetric, which means that $KL(P||Q) \neq KL(Q||P)$. So we introduce the JS divergence:

\begin{equation}
    JS(P||Q) = \frac{1}{2}KL(P||M) + \frac{1}{2}KL(Q||M)
\end{equation}

where $M = \frac{1}{2}(P + Q)$. The range of JS divergence is $[0, log2]$. JS divergence is obviously symmetric and the smaller the JS divergence is, the more similar two distributions are.

\subsection{Definitions in GANs}

\begin{itemize}
    \item \textbf{Generator G} - G is a function that takes a vector $z$ and outputs $x$. So given a prior distribution $P_{z}(z)$, then a probability distribution $P_{G}(x)$ is defined by function G.
    \item \textbf{Discriminator D} - D is a function (actually a binary classification) takes $x$ as inputs and outputs a scalar in range of $[0, 1]$ to tell the input $x$ is faked or not.
    \item \textbf{V(G, D)} - The function V(G, D) is defined to be:
        \begin{equation}
            V(G, D) = E_{x\sim p_{data}}[log(D(x))]+E_{z\sim p_z}[log(1-D(G(z)))]
        \end{equation}
        Defining V(G, D) like this allows the following missions:
            \begin{enumerate}
                \item Given a G, $\max_{D}V(G,D)$ evaluates the difference between $p_G$ and  $p_{data}$.
                \item $\min \limits_{G} \max \limits_{D}V(G, D)$ picks a generator that minimize the difference.
            \end{enumerate}
\end{itemize}

\subsection{Solve $\mathop{\arg\max} \limits_{D}V(G, D)$}

Given a G, thus distribution $p_{G}$ is determined. We have

\begin{equation}
    \begin{split}
        V & = E_{x\sim p_{data}}[log(D(x))]+E_{z\sim p_z}[log(1-D(G(z)))] \\
          & = E_{x\sim p_{data}}[log(D(x))]+E_{x\sim p_G}[log(1-D(x))] \\
          & = \int_{x}p_{data}(x)log(D(x))dx + \int_{x}p_{G}(x)log(1-D(x))dx \\
          & = \int_{x}[p_{data}(x)log(D(x)) + p_{G}(x)log(1-D(x))]dx
    \end{split}
\end{equation}

Because $p_{data}$ and $p_G$ are both fixed, for each input $x$, we can easily compute the result that maximizes $V(G,D)$:

\begin{equation}
    D^{*}(x) = \frac{p_{data}(x)}{p_{data}(x)+p_{G}(x)}
\end{equation}

\subsection{Why $\max \limits_{D} V(G,D)$ evaluates the difference of distributions?}

\begin{equation}
    \begin{split}
        & \quad \max \limits_{D}V(G,D) \\
        & = V(G,D^{*}) \\
        & = E_{x \sim p_{data}}[log\frac{p_{data}(x)}{p_{data}(x)+p_{G}(x)}] + E_{x \sim p_{G}}[log\frac{p_{G}(x)}{p_{data}(x)+p_{G}(x)}] \\
        & = \int_{x}p_{data}(x)log\frac{p_{data}(x) \times \frac{1}{2}}{(p_{data}(x)+p_{G}(x)) \times \frac{1}{2}}dx \\
        & \quad \ \  + \int_{x}p_{G}(x)log\frac{p_{G}(x)}{\frac{1}{2}(p_{data}(x)+p_{G}(x)) \times \frac{1}{2}}dx \\
        & = -2log2 + \int_{x}p_{data}(x)log\frac{p_{data}(x)}{\frac{1}{2}(p_{data}(x)+p_{G}(x))}dx \\
        & \quad \ \  + \int_{x}p_{G}(x)log\frac{p_{G}(x)}{\frac{1}{2}(p_{data}(x)+p_{G}(x))}dx \\
        & = -2log2 + KL(p_{data}||\frac{p_{data}+p_G}{2} + KL(p_{G}||\frac{p_{data}+p_G}{2}) \\
        & = -2log2 + JS(p_{data}||p_G)
    \end{split}
\end{equation}

Therefore, we can see that $V(G,D^{*})$ is the sum of a JS divergence and a constant $-2log2$. So it can evaluate the difference of distributions. And the smaller the $V(G,D^{*})$ is, the more similar $p_{data}$ and $p_G$ are.

\subsection{Final target - the generator}

Our final aim is to achieve a generator G that have the same distribution $p_G$ as $p_{data}$, which means the smallest divergence. So our optimal objective is

\begin{equation}
    G^{*} = \arg \min \limits_{G} \max \limits_{D} V(G,D)
\end{equation}

\section{Tips for implementing a GAN}

\begin{enumerate}
    \item Due to the difficulty of computing the real expectation of distributions, we use the arithmetic mean of samples $\{x^(1),...,x^{(m)}\}$ and $\{z^{(1)},...,z^{(m)}\}$ to approximate it:
        \begin{equation}
            \frac{1}{m}\sum_{i=1}^{m}[log(D(x^{(i)}))+log(1-D(G(z^{(i)})))]
        \end{equation}
    \item Update the generator by descending the following stochastic gradient can be inefficient:
        \begin{equation}
            \nabla_{\theta_{G}}\frac{1}{m}\sum_{i=1}^{m}log(1-D(G(z^{(i)})))
        \end{equation}
    Because the equation indicates that the more real the generated image $z$ is, the smaller the gradient is. That is, the gradient will be almost $0$ at the begin of training and increase significantly when the generator is able to generate a good enough image! This will make the training inefficient and uneasy to converge.
    So we usually use the gradient below as an alternative. More real the image, less the gradient.
        \begin{equation}
            \nabla_{\theta_{G}}\frac{1}{m}\sum_{i=1}^{m}log(D(G(z^{(i)})))
        \end{equation}
\end{enumerate}

\section{Works to do next week}

Next week, I will start training a GAN using the lab's GPU server. I have already obtain my account but I am not familiar with the operations to work remotely using a terminal. So I may spend some time to learn how to use the GPU server. 

\end{document}